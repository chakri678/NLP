{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lab 01: Perform Tokenization, Stemming, Lemmatization and Sentence Segmentation"
      ],
      "metadata": {
        "id": "0_TOJmDdPQLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text data\n",
        "textE='''\n",
        "Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interaction between computers and humans in natural language. The ultimate goal of NLP is to help computers understand language as well as we do. It is the driving force behind things like virtual assistants, speech recognition, sentiment analysis, automatic text summarization, machine translation and much more.\n",
        "The most important basic concepts that power NLP techniques include lemmatization, stemming, tokenization, and sentence segmentation. These are all important techniques to train efficient and effective NLP models.\n",
        "'''\n",
        "textH='''आर्टिफिशियल इंटेलिजेंस की शुरुआत साल 1950 में हुई थी। यह ह्यूमन कंप्यूटर इंटरेक्शन है।'''\n",
        "textT='''కృత్రిమ మేధస్సు 1950 సంవత్సరంలో ప్రారంభమైంది. ఇది మానవ కంప్యూటర్ పరస్పర చర్య.'''"
      ],
      "metadata": {
        "id": "AXS1lb97O9AB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "q9LKiU_57c5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## imort the required libraries and if needed install\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QdkOiVM6Qe9C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h86ipNmiXC0w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "edqrQW-9TcsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the Word Tokenization using split()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LY50JDbTXC72"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the Sentence Tokenization using split()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UtvTtuMiRWRf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the Word Tokenization using NLTK\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k_Zx1nK_RZgy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the sentence Tokenization using NLTK\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pKeAXbL8ReM5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the word Tokenization using spacy\n",
        "\n",
        "## import the libraries\n",
        "\n",
        "## load the language model\n",
        "\n",
        "## tokenization\n"
      ],
      "metadata": {
        "id": "PUXLUdsJROLo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the sentence Tokenization using spacy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QDM_oE5pR-ZM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords from English Text\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=stopwords.words('english')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "385gBL_qSMlA",
        "outputId": "4eb3ae64-f826-45a8-94fb-d69faace8f3b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "ZdP8-mxuTYko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TextS=\"eating eats ate eaten running  walking met meeting\" "
      ],
      "metadata": {
        "id": "nwzkBNX5WAFn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform stemming using Porter Stemmer\n"
      ],
      "metadata": {
        "id": "hjClOgdhSpGw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform stemming using Snowball Stemmer\n"
      ],
      "metadata": {
        "id": "wEl1L44zTCEB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform stemming using Lancaster Stemmer\n"
      ],
      "metadata": {
        "id": "lE-r8rspTMGo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "suAUuI2YTU5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Lemmatization using NLTK\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#Download\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61LY9ZFCThqQ",
        "outputId": "56055fbd-3816-4186-e5f3-2f6da189ac73"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Lemmatization using Spacy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FXYWSH6rUHdB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Segmentation"
      ],
      "metadata": {
        "id": "YYEGuUPnUrCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform sentence segmentation using native python code\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jCwxSaOoUvoQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform sentence segmentation using NLTK\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mzjZnNuqU4tZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform sentence segmentation using Spacy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ooF9vHehVAVQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the custom sentence segmentation using spacy on the following text with sentence bounday '.' and ';'\n",
        "textC='''\n",
        "India is the largest democratic country; It is a big country divided into 29 states and 7 union territories; These states and union territories have been created so that the government can run the country more easily; India also has many different kinds of physical features in different parts of the country that are spread over its states and union territories.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nlp.add_pipe('custom_boundary',before='parser')\n",
        "\n"
      ],
      "metadata": {
        "id": "wLqNK2ExVUKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "id": "evx1Ub5JYBjJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}